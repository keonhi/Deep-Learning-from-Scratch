{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def softmax(a):\n",
    "    c=np.max(a)\n",
    "    exp_a=np.exp(a-c)\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta)) #log안이 0이 되는 것을 막기 위해서\n",
    "\n",
    "def numerical_gradient(f,x): #수치미분\n",
    "    h=1e-4 #1e-4 정도면 좋은 결과를 얻는다고 알려져 있다\n",
    "    grad=np.zeros_like(x) #x와 같은 형상의 np.array를 만들고 모든 원소는 0이다\n",
    "    if len(x.shape)==2:\n",
    "        (row,column) =x.shape\n",
    "        for i in range(row):\n",
    "            for j in range(column):\n",
    "                tmp_val=x[i][j]\n",
    "                x[i][j]=tmp_val+h\n",
    "                fxh1=f(x)\n",
    "            \n",
    "                x[i][j]=tmp_val-h\n",
    "                fxh2=f(x)\n",
    "            \n",
    "                grad[i][j]=(fxh1-fxh2)/(2*h)\n",
    "                x[i][j]=tmp_val\n",
    "    \n",
    "        \"\"\"\n",
    "        xh1=x\n",
    "        xh2=x #이렇게 변수끼리 서로 같게 놓아버리면 간섭이 발생하는 것 같다.\n",
    "        xh1[i]+=h\n",
    "        xh2[i]-=h \n",
    "        f_temp=(f(xh1)-f(xh2))/(2*h)\n",
    "        grad[i]=f_temp\n",
    "        \"\"\"\n",
    "    else: \n",
    "        for i in range(x.size):\n",
    "            tmp_val=x[i]\n",
    "            x[i]=tmp_val+h\n",
    "            fxh1=f(x)\n",
    "            \n",
    "            x[i]=tmp_val-h\n",
    "            fxh2=f(x)\n",
    "            \n",
    "            grad[i]=(fxh1-fxh2)/(2*h)\n",
    "            x[i]=tmp_val\n",
    "    return grad\n",
    "\n",
    "class TwoLayerNetwork:\n",
    "    def __init__ (self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2=self.params['W1'],self.params['W2']\n",
    "        b1, b2=self.params['b1'],self.params['b2']\n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        max_y=np.argmax(y, axis=1) #axis=1은 가로로 즉 한 행에서 비교. axis=2는 세로로 즉 한 열에서 비교\n",
    "        #argmax는 최댓값이 있는 위치 인덱스들을 (axis=1, or 0의 경우) array의 형태로 가져다 준다. axis가 없으면 그냥 처음부터 세서 숫자.\n",
    "        return np.sum(max_y==t)/float(x.shape[0]) #max_y=np.array([1,2,3,4]) t=np.array([1,0,3,0]) max_y==t는 array([ True, False,  True, False]).\n",
    "\n",
    "    def numerical_gradients(self, x, t):\n",
    "        loss_w = lambda w: network.loss(x,t)\n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_w, self.params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_w, self.params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_w, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(100,784)\n",
    "t=np.random.randn(100,10)\n",
    "network=TwoLayerNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "y=network.predict(x)\n",
    "grads=network.numerical_gradients(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "import time\n",
    "start=time.time()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test)= load_mnist(normalize=True, one_hot_label=True)\n",
    "train_size= x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.05\n",
    "network=TwoLayerNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num=100\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_points=np.random   .choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_points]\n",
    "    t_batch=t_train[batch_points]\n",
    "    \n",
    "    grad=network.numerical_gradients(x_batch, t_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "\n",
    "    loss=network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    print(str(i+1)+\"th trial: \"+str(loss))\n",
    "    if ((i+1) % iter_per_epoch)==0:\n",
    "        train_acc=network.accuracy(x_train, t_train)\n",
    "        test_acc=network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc:\"+str(train_acc)+\",\"+str(test_acc))\n",
    "    print(str((time.time()-start)/60)+\" minutes passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__ (self):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self, x, y):\n",
    "        self.x=x #여기서 self.x, self.y 값이 갱신되어 뒤에서 backward에 사용됨.\n",
    "        self.y=y\n",
    "        return x*y\n",
    "    def backward(self, dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return (dx, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple=100\n",
    "apple_num=2\n",
    "tax=1.1\n",
    "\n",
    "apple_mullayer=MulLayer()\n",
    "tax_mullayer=MulLayer()\n",
    "\n",
    "apple_price_before_tax=apple_mullayer.forward(apple, apple_num)\n",
    "apple_price=tax_mullayer.forward(apple_price_before_tax, tax)\n",
    "\n",
    "print(apple_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dapple_price=1\n",
    "dapple_price_before_tax, dtax =tax_mullayer.backward(dapple_price)\n",
    "dapple, dapple_num =apple_mullayer.backward(dapple_price_before_tax)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        self.x\n",
    "        self.y\n",
    "    def forward(self, x, y):\n",
    "        return x+y\n",
    "    def backward(self, dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "650.0 2.2 110.00000000000001 3.3000000000000003 165.0\n"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "    def __init__ (self):\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "    def forward(self, x, y):\n",
    "        self.x=x #여기서 self.x, self.y 값이 갱신되어 뒤에서 backward에 사용됨.\n",
    "        self.y=y\n",
    "        return x*y\n",
    "    def backward(self, dout):\n",
    "        dx=dout*self.y\n",
    "        dy=dout*self.x\n",
    "        return (dx, dy)\n",
    "    \n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        return x+y\n",
    "    def backward(self, dout):\n",
    "        dx=dout*1\n",
    "        dy=dout*1\n",
    "        return dx, dy\n",
    "\n",
    "apple_price_one=100\n",
    "orange_price_one=150\n",
    "apple_num=2\n",
    "orange_num=3\n",
    "tax=1.1\n",
    "\n",
    "apple_mullayer=MulLayer()\n",
    "orange_mullayer=MulLayer()\n",
    "fruits_addlayer=AddLayer()\n",
    "tax_mullayer=MulLayer()\n",
    "\n",
    "apple_price=apple_mullayer.forward(apple_price_one, apple_num)\n",
    "orange_price=orange_mullayer.forward(orange_price_one, orange_num)\n",
    "fruits_price=fruits_addlayer.forward(apple_price, orange_price)\n",
    "final_price=tax_mullayer.forward(fruits_price, tax)\n",
    "\n",
    "#backpropagation\n",
    "dfruits_price, dtax=tax_mullayer.backward(1.0)\n",
    "dapple_price, dorange_price=fruits_addlayer.backward(dfruits_price)\n",
    "dapple_price_one, dapple_num=apple_mullayer.backward(dapple_price)\n",
    "dorange_price_one, dorange_num=orange_mullayer.backward(dorange_price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(final_price)\n",
    "\n",
    "print(dtax,dapple_price_one,dapple_num,dorange_price_one,dorange_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask=(x<=0)\n",
    "        out=x.copy()  #변수끼리 등호로 연결해버리면 그 둘 중 어느 하나에 영향을 주면 다른 것도 똑같이 영향을 받게 된다.\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask]=0\n",
    "        dx=dout\n",
    "        return dx\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out=None\n",
    "    def forward(self, x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        return dout*self.out*(1-self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        return np.dot(x,self.W)+self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx=np.dot(dout,self.W.T)\n",
    "        self.dW=np.dot(self.x.T,dout)\n",
    "        self.dB=np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss=None\n",
    "        self.y=None\n",
    "        self.t=None\n",
    "    def forward(self,x,t):\n",
    "        self.t=t\n",
    "        self.y=softmax(x)\n",
    "        self.loss=cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx=(self.y-self.t)/batch_size\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNetwork:\n",
    "    def __init__ (self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "    \n",
    "        self.layers=OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1']=Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'],self.params['b2'])\n",
    "        self.lastLayer=SoftmaxWithLoss()\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x=layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        max_y=np.argmax(y, axis=1) #axis=1은 가로로 즉 한 행에서 비교. axis=2는 세로로 즉 한 열에서 비교\n",
    "        #argmax는 최댓값이 있는 위치 인덱스들을 (axis=1, or 0의 경우) array의 형태로 가져다 준다. axis가 없으면 그냥 처음부터 세서 숫자.\n",
    "        return np.sum(max_y==t)/float(x.shape[0]) #max_y=np.array([1,2,3,4]) t=np.array([1,0,3,0]) max_y==t는 array([ True, False,  True, False]).\n",
    "\n",
    "    def numerical_gradients(self, x, t):\n",
    "        loss_w = lambda w: network.loss(x,t)\n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_w, self.params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_w, self.params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_w, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x,t)\n",
    "        dout=1\n",
    "        dout=self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers=list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout=layer.backward(dout)\n",
    "        grads={}\n",
    "        grads['W1']=self.layers['Affine1'].dW\n",
    "        grads['b1']=self.layers['Affine1'].db\n",
    "        grads['W2']=self.layers['Affine2'].dW\n",
    "        grads['b2']=self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-2a8573656386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "import time\n",
    "start=time.time()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test)= load_mnist(normalize=True, one_hot_label=True)\n",
    "train_size= x_train.shape[0]\n",
    "batch_size=1\n",
    "learning_rate=0.1\n",
    "network=TwoLayerNetwork(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num=100\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_points=np.random   .choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_points]\n",
    "    t_batch=t_train[batch_points]\n",
    "    \n",
    "    grad=network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "\n",
    "    loss=network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    print(str(i+1)+\"th trial: \"+str(loss))\n",
    "    if ((i+1) % iter_per_epoch)==0:\n",
    "        train_acc=network.accuracy(x_train, t_train)\n",
    "        test_acc=network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc:\"+str(train_acc)+\",\"+str(test_acc))\n",
    "    print(str((time.time()-start)/60)+\" minutes passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
